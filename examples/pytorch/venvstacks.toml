# Demonstrate using priority indexes to define cross-platform parallel torch stacks
[[runtimes]]
name = "cpython3.11"
python_implementation = "cpython@3.11.13"
requirements = [
    # Share a common numpy across the different torch variants
    "numpy",
]

[[frameworks]]
name = "torch-cpu"
runtime = "cpython3.11"
package_indexes = { torch = "pytorch-cpu" }
# priority_indexes = ["pytorch-cpu"]
requirements = [
    "torch==2.8.0",
    # Skip listing numpy, so numpy updates don't automatically invalidate the layer lock
]
dynlib_exclude = [
    "triton/**"
]
platforms = [
    "linux_aarch64",
    "linux_x86_64",
    "macosx_arm64",
    # "macosx_x86_64",  # PyTorch does not publish wheels for macOS on Intel
    "win_amd64",
    "win_arm64",
]

[[frameworks]]
name = "torch-cu128"
runtime = "cpython3.11"
package_indexes = { torch = "pytorch-cu128" }
# priority_indexes = ["pytorch-cu128"]
requirements = [
    "torch==2.8.0",
    # Skip listing numpy, so numpy updates don't automatically invalidate the layer lock
]
dynlib_exclude = [
    "triton/**"
]
platforms = [
    # "linux_aarch64",  # No wheel available in the PyTorch 2.8.0 CUDA repo
    "linux_x86_64",
    # "macosx_arm64",  # CUDA is not used on macOS
    # "macosx_x86_64",  # PyTorch does not publish wheels for macOS on Intel
    "win_amd64",
    # "win_arm64",  # No wheel available in the PyTorch 2.8.0 CUDA repo
]

[[applications]]
name = "cpu"
launch_module = "report_torch_cuda_version.py"
frameworks = ["torch-cpu"]
requirements = [
    # Exact version pin is inherited from the framework layer
    "torch",
]

[[applications]]
name = "cu128"
launch_module = "report_torch_cuda_version.py"
frameworks = ["torch-cu128"]
requirements = [
    # Exact version pin is inherited from the framework layer
    "torch",
]

[[applications]]
name = "cu128-or-cpu"
launch_module = "report_torch_cuda_version.py"
# Both the CUDA and non-CUDA frameworks are added to the import path,
# so this app will work as long as *either* of those layers is installed
# If both are available, it uses the CUDA layer (as it is listed first)
# However, the layer locking needs to be told that it is expected that
# the two layers specify different source indexes, and given a conflict,
# the pytorch-cu128 index should be used in preference to pytorch-cpu
index_overrides = { pytorch-cpu = "pytorch-cu128" }
frameworks = ["torch-cu128", "torch-cpu"]
requirements = [
    # Exact version pin is inherited from the framework layer
    "torch",
]


[tool.uv]
# exclude-newer = "2025-10-11T00:00:00Z"
# The custom torch registries do not support exclude-newer,
# and that currently requires avoiding the feature entirely
# https://github.com/astral-sh/uv/issues/12449

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu/"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128/"
explicit = true
