# Demonstrate using priority indexes to define cross-platform parallel torch stacks
[[runtimes]]
name = "cpython3.11"
python_implementation = "cpython@3.11.13"
requirements = [
    # Share a common numpy across the different torch variants
    "numpy",
]

[[frameworks]]
name = "torch-cpu"
runtime = "cpython3.11"
package_indexes = { torch = "pytorch-cpu" }
# priority_indexes = ["pytorch-cpu"]
requirements = [
    "torch==2.8.0",
    # Skip listing numpy, so numpy updates don't automatically invalidate the layer lock
]
dynlib_exclude = [
    "triton/**"
]

[[frameworks]]
name = "torch-cu128"
runtime = "cpython3.11"
package_indexes = { torch = "pytorch-cu128" }
# priority_indexes = ["pytorch-cu128"]
requirements = [
    "torch==2.8.0",
    # Skip listing numpy, so numpy updates don't automatically invalidate the layer lock
]
dynlib_exclude = [
    "triton/**"
]

[[applications]]
name = "cpu"
launch_module = "report_torch_cuda_version.py"
frameworks = ["torch-cpu"]
# Skip listing deps, so framework updates don't automatically invalidate the layer lock
requirements = []

[[applications]]
name = "cu128"
launch_module = "report_torch_cuda_version.py"
frameworks = ["torch-cu128"]
# Skip listing deps, so framework updates don't automatically invalidate the layer lock
requirements = []

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu/"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128/"
explicit = true
